import os
import tensorflow as tf

# Metadata dosyasının yolu
metadata_file = "C:/Users/win11/Desktop/volodymyr labs(NN)/lab8_deneme/LJSpeech-1.1/metadata.csv"

# Ses dosyalarının bulunduğu dizin yolu
wavs_path = "C:/Users/win11/Desktop/volodymyr labs(NN)/lab8_deneme/LJSpeech-1.1/wavs/"

# Dosya yolunu doğrulama
if not os.path.isfile(metadata_file):
    print("Hata: metadata.csv dosyası bulunamadı.")
    exit()

# Dosyayı açma ve içeriği okuma (utf-8 kodlaması kullanarak)
with open(metadata_file, "r", encoding="utf-8") as file:
    metadata = file.readlines()

# Veriyi işleme
data = []
for line in metadata:
    parts = line.strip().split("|")
    if len(parts) == 2:  # Beklenen format: dosya_adı | metin
        file_name, text = parts
        # Metni düşük harfe dönüştürme ve gereksiz karakterleri temizleme
        text = text.strip().lower()
        text = text.replace(",", "").replace(".", "").replace(";", "")
        data.append((file_name, text))

# Veriyi eğitim, doğrulama ve test kümelerine ayırma
train_data = data[:int(len(data)*0.8)]
val_data = data[int(len(data)*0.8):int(len(data)*0.9)]
test_data = data[int(len(data)*0.9):]

# Veri kümesini oluşturma işlemi
def create_dataset(data):
    file_paths = [os.path.join(wavs_path, file_name + ".wav") for file_name, _ in data]
    labels = [text for _, text in data]
    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))
    return dataset

# Eğitim, doğrulama ve test veri kümelerini oluşturma
train_dataset = create_dataset(train_data).shuffle(buffer_size=len(train_data)).batch(32)
val_dataset = create_dataset(val_data).batch(32)
test_dataset = create_dataset(test_data).batch(32)

# Model oluşturma
model = tf.keras.Sequential([
    # Model katmanlarını buraya ekleyin
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10)
])

# Modeli derleme
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Modeli eğitme
model.fit(train_dataset, validation_data=val_dataset, epochs=10)

# Modeli değerlendirme
loss, accuracy = model.evaluate(test_dataset)
print(f"Test veri kümesi üzerinde kayıp: {loss}, doğruluk: {accuracy}")

# Modeli test etme (opsiyonel)
predictions = model.predict(test_dataset)
